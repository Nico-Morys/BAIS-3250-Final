{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72c289bb-7e71-458d-a2cc-8462d17d62b1",
   "metadata": {},
   "source": [
    "### Jacob Kopec and Nico Morys\n",
    "### Data Wrangling Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dffd988-8fdc-4a2f-8846-fa7bd005990a",
   "metadata": {},
   "source": [
    "### This code is for scraping the data of the Top 1000 Highest-Grossing Movies of All Time from IMDb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b39fa3-fb87-4b42-834f-7bc84e13b0a2",
   "metadata": {},
   "source": [
    "#### The data was extracted from this website https://www.imdb.com/list/ls098063263/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e6dc92a-a938-4f79-bf62-98fe07b74717",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import re\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8dd8262e-72eb-49ba-b070-97008983c66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize WebDriver and open the URL\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "driver.get(\"https://www.imdb.com/list/ls098063263/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "250ae577-d199-44a5-8d8d-4deb356d41e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scrolling to the bottom of the page to load all movies...\n",
      "Finished scrolling to the bottom. Starting scraping...\n",
      "Scraping Page 1...\n",
      "Total unique movies scraped: 250\n",
      "Waiting 8 seconds before next scroll...\n",
      "Scraping Page 2...\n",
      "Total unique movies scraped: 250\n",
      "Waiting 6 seconds before next scroll...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 126\u001b[0m\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m imdb_data\n\u001b[0;32m    125\u001b[0m \u001b[38;5;66;03m# Usage\u001b[39;00m\n\u001b[1;32m--> 126\u001b[0m imdb_df \u001b[38;5;241m=\u001b[39m scroll_and_scrape_imdb(driver)\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28mprint\u001b[39m(imdb_df\u001b[38;5;241m.\u001b[39mhead())\n",
      "Cell \u001b[1;32mIn[7], line 104\u001b[0m, in \u001b[0;36mscroll_and_scrape_imdb\u001b[1;34m(driver)\u001b[0m\n\u001b[0;32m    102\u001b[0m wait_time \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWaiting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwait_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds before next scroll...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 104\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(wait_time)\n\u001b[0;32m    106\u001b[0m \u001b[38;5;66;03m# Increment the page number\u001b[39;00m\n\u001b[0;32m    107\u001b[0m page_number \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def scroll_and_scrape_imdb(driver):\n",
    "    # Initialize data lists\n",
    "    titles, release_years, ratings, gross_earnings, directors = [], [], [], [], []\n",
    "\n",
    "    def scroll_to_bottom(driver):\n",
    "        #Scroll the page to the very bottom to ensure all content is loaded.\n",
    "        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        while True:\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(3)  # Wait for content to load\n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                break\n",
    "            last_height = new_height\n",
    "\n",
    "    def ensure_unique_movies(current_titles, scraped_titles):\n",
    "        #Ensure the scraped titles are unique to avoid duplicates.\n",
    "        unique_indexes = [i for i, title in enumerate(scraped_titles) if title not in current_titles]\n",
    "        return unique_indexes\n",
    "\n",
    "    # First, scroll to the bottom of the page to load all movies\n",
    "    print(\"Scrolling to the bottom of the page to load all movies...\")\n",
    "    scroll_to_bottom(driver)\n",
    "    print(\"Finished scrolling to the bottom. Starting scraping...\")\n",
    "\n",
    "    page_number = 1\n",
    "    total_movies_scraped = 0\n",
    "\n",
    "    while total_movies_scraped < 1000:\n",
    "        print(f\"Scraping Page {page_number}...\")\n",
    "\n",
    "        # Find all movie elements on the current page\n",
    "        movie_elements = driver.find_elements(By.XPATH, '//*[@id=\"__next\"]/main/div/section/div/section/div/div[1]/section/div[2]/ul/li')\n",
    "\n",
    "        if not movie_elements:\n",
    "            print(\"No more movie elements found. Ending scrape.\")\n",
    "            break\n",
    "\n",
    "        # Temporary lists for this round of scraping\n",
    "        temp_titles, temp_release_years, temp_ratings, temp_gross_earnings, temp_directors = [], [], [], [], []\n",
    "\n",
    "        for movie_element in movie_elements:\n",
    "            try:\n",
    "                # Title\n",
    "                title_element = movie_element.find_element(By.XPATH, \".//h3[@class='ipc-title__text']\")\n",
    "                temp_titles.append(title_element.text)\n",
    "\n",
    "                # Release Year\n",
    "                try:\n",
    "                    year_element = movie_element.find_element(By.XPATH, \".//span[contains(@class, 'dli-title-metadata-item') and (starts-with(text(), '20') or starts-with(text(), '19'))]\")\n",
    "                    temp_release_years.append(year_element.text.strip())\n",
    "                except Exception:\n",
    "                    temp_release_years.append(\"N/A\")\n",
    "\n",
    "                # Rating (e.g., PG, PG-13, etc.)\n",
    "                try:\n",
    "                    metadata_element = movie_element.find_element(By.XPATH, \".//div/div/div/div[1]/div[2]/div[2]\")\n",
    "                    metadata_text = metadata_element.text.split(\"\\n\")\n",
    "                    valid_ratings = [\"PG\", \"PG-13\", \"R\", \"TV-PG\", \"Not Rated\", \"G\", \"Approved\", \"TV-Y7\", \"TV-MA\"]\n",
    "                    movie_rating = next((item for item in metadata_text if item in valid_ratings), \"N/A\")\n",
    "                    temp_ratings.append(\"Not Rated\" if movie_rating in [\"N/A\", \"\", None] else movie_rating)\n",
    "                except Exception:\n",
    "                    temp_ratings.append(\"N/A\")\n",
    "\n",
    "                # Gross Earnings\n",
    "                try:\n",
    "                    gross_element = movie_element.find_element(By.XPATH, \".//span[contains(text(),'Worldwide Lifetime Gross:')]\")\n",
    "                    cleaned_gross = re.search(r'\\$([\\d,]+)', gross_element.text)\n",
    "                    temp_gross_earnings.append(cleaned_gross.group(1).replace(\",\", \"\") if cleaned_gross else \"N/A\")\n",
    "                except Exception:\n",
    "                    temp_gross_earnings.append(\"N/A\")\n",
    "\n",
    "                # Directors\n",
    "                try:\n",
    "                    director_elements = movie_element.find_elements(By.XPATH, \".//a[@class='ipc-link ipc-link--base dli-director-item']\")\n",
    "                    directors_list = [director.text for director in director_elements]\n",
    "                    temp_directors.append(\", \".join(directors_list))  # Join all directors with a comma\n",
    "                except Exception:\n",
    "                    temp_directors.append(\"N/A\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error while scraping a movie: {e}\")\n",
    "\n",
    "        # Filter only unique movies\n",
    "        unique_indexes = ensure_unique_movies(titles, temp_titles)\n",
    "        titles.extend([temp_titles[i] for i in unique_indexes])\n",
    "        release_years.extend([temp_release_years[i] for i in unique_indexes])\n",
    "        ratings.extend([temp_ratings[i] for i in unique_indexes])\n",
    "        gross_earnings.extend([temp_gross_earnings[i] for i in unique_indexes])\n",
    "        directors.extend([temp_directors[i] for i in unique_indexes])\n",
    "\n",
    "        # Update the total count of movies scraped\n",
    "        total_movies_scraped = len(titles)\n",
    "\n",
    "        print(f\"Total unique movies scraped: {total_movies_scraped}\")\n",
    "\n",
    "        # Break the loop if we've scraped 1000 movies or no more movies are loading\n",
    "        if total_movies_scraped >= 1000:\n",
    "            break\n",
    "\n",
    "        # Wait a random amount of time before scrolling again\n",
    "        wait_time = random.randint(5, 10)\n",
    "        print(f\"Waiting {wait_time} seconds before next scroll...\")\n",
    "        time.sleep(wait_time)\n",
    "\n",
    "        # Increment the page number\n",
    "        page_number += 1\n",
    "\n",
    "    # Split rank and title into separate columns\n",
    "    ranks = [int(title.split(\". \")[0]) if \". \" in title else None for title in titles]\n",
    "    cleaned_titles = [title.split(\". \", 1)[1] if \". \" in title else title for title in titles]\n",
    "\n",
    "    # Create a DataFrame from the scraped data\n",
    "    imdb_data = pd.DataFrame({\n",
    "        'rank': ranks,\n",
    "        'movie_name': cleaned_titles,\n",
    "        'release_year': pd.to_numeric(release_years, errors='coerce').astype('int'),\n",
    "        'audience_rating': pd.Categorical(ratings),\n",
    "        'gross_earnings': pd.to_numeric(gross_earnings, errors='coerce').astype('int64'),\n",
    "        'director(s)': pd.Series(directors, dtype='string')\n",
    "    })\n",
    "\n",
    "    return imdb_data\n",
    "\n",
    "# Usage\n",
    "imdb_df = scroll_and_scrape_imdb(driver)\n",
    "print(imdb_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f587ff-b078-462b-b2be-34eb737f26b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(imdb_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b73b99-4a89-4f8c-aff9-dcbf1926731e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data to a CSV\n",
    "imdb_df.to_csv(\"IMDbHG_raw.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8228c09-c0c5-4df3-bd7a-9275fc054e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_df.dtypes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
